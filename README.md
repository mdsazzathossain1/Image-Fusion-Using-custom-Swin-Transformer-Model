# Image-Fusion-Using-custom-Swin-Transformer-Model
Trains/evaluates a PyTorch model to fuse infrared and visible images using a custom encoder–decoder with transformer-style fusion blocks (Swin/ViT-like attention). Evaluates fused outputs with SSIM and PSNR; includes a dataset class that loads paired IR/Visible images.
A PyTorch implementation of infrared–visible image fusion built around a lightweight encoder–transformer–decoder architecture that mimics Swin/ViT-style attention for feature mixing: it expects paired and spatially aligned images in data/infrared/{train,val} and data/visible/{train,val} with matching filenames (e.g., 001.jpg present in both modalities), constructs a custom InfraredVisibleDataset with torchvision transforms, instantiates the model (CNN encoders extract modality-specific features, windowed/self-attention modules fuse them, and a decoder reconstructs a sharp fused output), and trains with a composite loss (e.g., SSIM + intensity/content) using Adam while logging per-epoch loss. After training, it evaluates on the validation set with PSNR and SSIM, writes a few fused samples to output/, and optionally saves checkpoints to checkpoints/. You can control DATA_DIR, IMG_SIZE, BATCH_SIZE, EPOCHS, and DEVICE="cuda" for GPU runs; this script is ideal when you need a reproducible baseline for multi-sensor fusion and objective quality metrics. Required packages are torch, torchvision, pillow, opencv-python, scikit-image, sewar, pytorch-ssim, numpy, pandas, and matplotlib.
